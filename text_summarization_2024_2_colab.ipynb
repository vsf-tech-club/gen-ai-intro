{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Text Summarization with Generative Models on Vertex AI\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/prompts/examples/text_summarization.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/prompts/examples/text_summarization.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/language/prompts/examples/text_summarization.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "Text summarization produces a concise and fluent summary of a longer text document. There are two main text summarization types: extractive and abstractive. Extractive summarization involves selecting critical sentences from the original text and combining them to form a summary. Abstractive summarization involves generating new sentences representing the original text's main points. In this notebook, you go through a few examples of how large language models can help with generating summaries based on text.\n",
        "\n",
        "Learn more about text summarization in the [official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/text/summarization-prompts)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d975e698c9a4"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you will learn how to use generative models to summarize information from text by working through the following examples:\n",
        "- Transcript summarization\n",
        "- Summarizing text into bullet points\n",
        "- Dialogue summarization with to-dos\n",
        "- Hashtag tokenization\n",
        "- Title & heading generation\n",
        "\n",
        "You also learn how to evaluate model-generated summaries by comparing to human-created summaries using ROUGE as an evaluation framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6d865e68adb"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI Generative AI Studio\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
        "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs9TZo0GJKCR"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a5AEr0lkLKD"
      },
      "source": [
        "### Install Vertex AI SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "148dd6321946",
        "outputId": "ad40f127-6815-474b-a55f-6caf83727479"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting google-cloud-aiplatform\n",
            "  Obtaining dependency information for google-cloud-aiplatform from https://files.pythonhosted.org/packages/46/d9/280e5a9b5caf69322f64fa55f62bf447d76c5fe30e8df6e93373f22c4bd7/google_cloud_aiplatform-1.70.0-py2.py3-none-any.whl.metadata\n",
            "  Downloading google_cloud_aiplatform-1.70.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from google-cloud-aiplatform) (2.21.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from google-cloud-aiplatform) (2.35.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from google-cloud-aiplatform) (1.25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from google-cloud-aiplatform) (5.28.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from google-cloud-aiplatform) (24.1)\n",
            "Collecting google-cloud-storage<3.0.0dev,>=1.32.0 (from google-cloud-aiplatform)\n",
            "  Obtaining dependency information for google-cloud-storage<3.0.0dev,>=1.32.0 from https://files.pythonhosted.org/packages/fc/da/95db7bd4f0bd1644378ac1702c565c0210b004754d925a74f526a710c087/google_cloud_storage-2.18.2-py2.py3-none-any.whl.metadata\n",
            "  Downloading google_cloud_storage-2.18.2-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 (from google-cloud-aiplatform)\n",
            "  Obtaining dependency information for google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 from https://files.pythonhosted.org/packages/2a/91/e1c80ae2924efc047ca156662d6b0458d9a9ce99204ae7e719ff9a66123d/google_cloud_bigquery-3.26.0-py2.py3-none-any.whl.metadata\n",
            "  Downloading google_cloud_bigquery-3.26.0-py2.py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform)\n",
            "  Obtaining dependency information for google-cloud-resource-manager<3.0.0dev,>=1.3.3 from https://files.pythonhosted.org/packages/c3/c2/afdfd9b2d140d77091185a405aa6d91a6d74568862c54e83bf04de3b9693/google_cloud_resource_manager-1.13.0-py2.py3-none-any.whl.metadata\n",
            "  Downloading google_cloud_resource_manager-1.13.0-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting shapely<3.0.0dev (from google-cloud-aiplatform)\n",
            "  Obtaining dependency information for shapely<3.0.0dev from https://files.pythonhosted.org/packages/37/63/e182e43081fffa0a2d970c480f2ef91647a6ab94098f61748c23c2a485f2/shapely-2.0.6-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
            "  Downloading shapely-2.0.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: pydantic<3 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from google-cloud-aiplatform) (2.9.2)\n",
            "Collecting docstring-parser<1 (from google-cloud-aiplatform)\n",
            "  Obtaining dependency information for docstring-parser<1 from https://files.pythonhosted.org/packages/d5/7c/e9fcff7623954d86bdc17782036cbf715ecab1bec4847c008557affe1ca8/docstring_parser-0.16-py3-none-any.whl.metadata\n",
            "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.65.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.67.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.67.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
            "Collecting google-cloud-core<3.0.0dev,>=2.4.1 (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform)\n",
            "  Obtaining dependency information for google-cloud-core<3.0.0dev,>=2.4.1 from https://files.pythonhosted.org/packages/5e/0f/2e2061e3fbcb9d535d5da3f58cc8de4947df1786fe6a1355960feb05a681/google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata\n",
            "  Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting google-resumable-media<3.0dev,>=2.0.0 (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform)\n",
            "  Obtaining dependency information for google-resumable-media<3.0dev,>=2.0.0 from https://files.pythonhosted.org/packages/82/35/b8d3baf8c46695858cb9d8835a53baa1eeb9906ddaf2f728a5f5b640fd1e/google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata\n",
            "  Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.3 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n",
            "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4 (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform)\n",
            "  Obtaining dependency information for grpc-google-iam-v1<1.0.0dev,>=0.12.4 from https://files.pythonhosted.org/packages/a8/7d/da3875b7728bc700eeb28b513754ce237c04ac7cbf8559d76b0464ee01cb/grpc_google_iam_v1-0.13.1-py2.py3-none-any.whl.metadata\n",
            "  Downloading grpc_google_iam_v1-0.13.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform)\n",
            "  Obtaining dependency information for google-crc32c<2.0dev,>=1.0 from https://files.pythonhosted.org/packages/7d/14/ab47972ac79b6e7b03c8be3a7ef44b530a60e69555668dbbf08fc5692a98/google_crc32c-1.6.0-cp311-cp311-macosx_12_0_arm64.whl.metadata\n",
            "  Downloading google_crc32c-1.6.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from pydantic<3->google-cloud-aiplatform) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from pydantic<3->google-cloud-aiplatform) (4.12.2)\n",
            "Requirement already satisfied: numpy<3,>=1.14 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from python-dateutil<3.0dev,>=2.7.3->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2024.8.30)\n",
            "Downloading google_cloud_aiplatform-1.70.0-py2.py3-none-any.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
            "Downloading google_cloud_bigquery-3.26.0-py2.py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.1/239.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_resource_manager-1.13.0-py2.py3-none-any.whl (359 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m359.9/359.9 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_storage-2.18.2-py2.py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shapely-2.0.6-cp311-cp311-macosx_11_0_arm64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
            "Downloading google_crc32c-1.6.0-cp311-cp311-macosx_12_0_arm64.whl (30 kB)\n",
            "Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpc_google_iam_v1-0.13.1-py2.py3-none-any.whl (24 kB)\n",
            "Installing collected packages: shapely, google-crc32c, docstring-parser, google-resumable-media, grpc-google-iam-v1, google-cloud-core, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform\n",
            "Successfully installed docstring-parser-0.16 google-cloud-aiplatform-1.70.0 google-cloud-bigquery-3.26.0 google-cloud-core-2.4.1 google-cloud-resource-manager-1.13.0 google-cloud-storage-2.18.2 google-crc32c-1.6.0 google-resumable-media-2.7.2 grpc-google-iam-v1-0.13.1 shapely-2.0.6\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#!pip install google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLVWFKFwkLKE"
      },
      "source": [
        "**Colab only:** Uncomment the following cell to restart the kernel or use the button to restart the kernel. For Vertex AI Workbench you can restart the terminal using the button on top."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_Hsqwn4hkLKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55b56856-bc51-4b38-e18e-33b8f7eaac5e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe7OuYuGkLKF"
      },
      "source": [
        "### Authenticating your notebook environment\n",
        "* If you are using **Colab** to run this notebook, uncomment the cell below and continue.\n",
        "* If you are using **Vertex AI Workbench**, check out the setup instructions [here](#TODO).\n",
        "* If you are using **local Jupyter**, check out the setup instructions [here](#TODO)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9Gx2SAZkLKF"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "960505627ddf"
      },
      "source": [
        "### Import libraries\n",
        "\n",
        "Let's start by importing the libraries that we will need for this tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pFdAGRR3SHy"
      },
      "source": [
        "**Colab only:** Uncomment the following cell to initialize the Vertex AI SDK. For Vertex AI Workbench, you don't need to run this.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfhHNJzq3SHy"
      },
      "outputs": [],
      "source": [
        "# import vertexai\n",
        "\n",
        "# PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "# vertexai.init(project=PROJECT_ID, location=\"us-central1\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "r6E_4_az42SM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "from vertexai.language_models import TextGenerationModel\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "#genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "\n",
        "generation_config = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 1024,\n",
        "    \"top_k\": 40,\n",
        "    \"top_p\": 0.8\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP76a2la7O-a"
      },
      "source": [
        "### Import models\n",
        "\n",
        "Here we load the pre-trained text generation model called `text-bison@001`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7isig7e07O-a"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel(\n",
        "  model_name=\"gemini-1.5-flash\",\n",
        "  generation_config=generation_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu1UAhoTKn51"
      },
      "source": [
        "## Text Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgZvJeBpJKCS"
      },
      "source": [
        "### Transcript summarization\n",
        "\n",
        "In this first example, you summarize a piece of text on quantum computing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UA2NjngeJKCS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "9b552ca5-53ac-4bfc-9e72-7f00ed096987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantum computers rely on qubits, which are extremely sensitive to errors.  To overcome this, quantum error correction is crucial, encoding information across multiple physical qubits to create a more stable \"logical qubit.\"  This approach aims to reduce error rates and enable the development of large-scale quantum computers capable of running useful algorithms. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Provide a very short summary, no more than three sentences, for the following article:\n",
        "\n",
        "Our quantum computers work by manipulating qubits in an orchestrated fashion that we call quantum algorithms.\n",
        "The challenge is that qubits are so sensitive that even stray light can cause calculation errors — and the problem worsens as quantum computers grow.\n",
        "This has significant consequences, since the best quantum algorithms that we know for running useful applications require the error rates of our qubits to be far lower than we have today.\n",
        "To bridge this gap, we will need quantum error correction.\n",
        "Quantum error correction protects information by encoding it across multiple physical qubits to form a “logical qubit,” and is believed to be the only way to produce a large-scale quantum computer with error rates low enough for useful calculations.\n",
        "Instead of computing on the individual qubits themselves, we will then compute on logical qubits. By encoding larger numbers of physical qubits on our quantum processor into one logical qubit, we hope to reduce the error rates to enable useful quantum algorithms.\n",
        "\n",
        "Summary:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aade04b2e86a"
      },
      "source": [
        "Instead of a summary, we can ask for a TL;DR (\"too long; didn't read\"). You can compare the differences between the outputs generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0c0c0f3dfe10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "e34c10d4-344b-45ec-c31b-f51efaa6f169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## TL;DR:\n",
            "\n",
            "Quantum computers are prone to errors due to their sensitive qubits. To overcome this, we need **quantum error correction**, which encodes information across multiple physical qubits to create a more robust \"logical qubit.\" This approach aims to reduce error rates and enable the development of large-scale, error-tolerant quantum computers capable of running useful algorithms. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Provide a TL;DR for the following article:\n",
        "\n",
        "Our quantum computers work by manipulating qubits in an orchestrated fashion that we call quantum algorithms.\n",
        "The challenge is that qubits are so sensitive that even stray light can cause calculation errors — and the problem worsens as quantum computers grow.\n",
        "This has significant consequences, since the best quantum algorithms that we know for running useful applications require the error rates of our qubits to be far lower than we have today.\n",
        "To bridge this gap, we will need quantum error correction.\n",
        "Quantum error correction protects information by encoding it across multiple physical qubits to form a “logical qubit,” and is believed to be the only way to produce a large-scale quantum computer with error rates low enough for useful calculations.\n",
        "Instead of computing on the individual qubits themselves, we will then compute on logical qubits. By encoding larger numbers of physical qubits on our quantum processor into one logical qubit, we hope to reduce the error rates to enable useful quantum algorithms.\n",
        "\n",
        "TL;DR:\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PATmHivJKCS"
      },
      "source": [
        "### Summarize text into bullet points\n",
        "In the following example, you use same text on quantum computing, but ask the model to summarize it in bullet-point form. Feel free to change the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2orkDF2VJKCT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "4e6d3507-6454-45c7-bde1-f8ce718da76e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are four bullet points summarizing the article:\n",
            "\n",
            "* **Quantum computers use qubits, which are extremely sensitive to errors.** Even stray light can disrupt calculations.\n",
            "* **Current qubit error rates are too high for practical quantum algorithms.**  \n",
            "* **Quantum error correction is essential for building large-scale quantum computers.** It encodes information across multiple physical qubits to create a more robust \"logical qubit.\"\n",
            "* **By encoding more physical qubits into a single logical qubit, researchers hope to reduce error rates and enable useful quantum algorithms.** \n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Provide a very short summary in four bullet points for the following article:\n",
        "\n",
        "Our quantum computers work by manipulating qubits in an orchestrated fashion that we call quantum algorithms.\n",
        "The challenge is that qubits are so sensitive that even stray light can cause calculation errors — and the problem worsens as quantum computers grow.\n",
        "This has significant consequences, since the best quantum algorithms that we know for running useful applications require the error rates of our qubits to be far lower than we have today.\n",
        "To bridge this gap, we will need quantum error correction.\n",
        "Quantum error correction protects information by encoding it across multiple physical qubits to form a “logical qubit,” and is believed to be the only way to produce a large-scale quantum computer with error rates low enough for useful calculations.\n",
        "Instead of computing on the individual qubits themselves, we will then compute on logical qubits. By encoding larger numbers of physical qubits on our quantum processor into one logical qubit, we hope to reduce the error rates to enable useful quantum algorithms.\n",
        "\n",
        "Bulletpoints:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE7y-clBJKCT"
      },
      "source": [
        "###  Dialogue summarization with to-dos\n",
        "Dialogue summarization involves condensing a conversation into a shorter format so that you don't need to read the whole discussion but can leverage a summary. In this example, you ask the model to summarize an example conversation between an online retail customer and a support agent, and include to-dos at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SV-BWzRhJKCT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "1e361e46-7a68-49f2-b2fa-96dc2bc393b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Larry contacted customer support to report receiving the wrong item. He requested a return and refund, which the support agent processed. The refund will be credited to Larry's account within 14 days.\n",
            "\n",
            "**To-Do's for the Support Agent:**\n",
            "\n",
            "* **Process the refund for Larry's order.**\n",
            "* **Ensure the refund is credited to Larry's account within 14 days.** \n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Please generate a summary of the following conversation and at the end summarize the to-do's for the support Agent:\n",
        "\n",
        "Customer: Hi, I'm Larry, and I received the wrong item.\n",
        "\n",
        "Support Agent: Hi, Larry. How would you like to see this resolved?\n",
        "\n",
        "Customer: That's alright. I want to return the item and get a refund, please.\n",
        "\n",
        "Support Agent: Of course. I can process the refund for you now. Can I have your order number, please?\n",
        "\n",
        "Customer: It's [ORDER NUMBER].\n",
        "\n",
        "Support Agent: Thank you. I've processed the refund, and you will receive your money back within 14 days.\n",
        "\n",
        "Customer: Thank you very much.\n",
        "\n",
        "Support Agent: You're welcome, Larry. Have a good day!\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlOgWzmNJKCT"
      },
      "source": [
        "###  Hashtag tokenization\n",
        "Hashtag tokenization is the process of taking a piece of text and getting the hashtag \"tokens\" out of it. You can use this, for example, if you want to generate hashtags for your social media campaigns. In this example, you take [this tweet from Google Cloud](https://twitter.com/googlecloud/status/1649127992348606469) and generate some hashtags you can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWa8rNV0JKCT",
        "outputId": "13943682-7044-4126-bc71-65c7dec01766"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tweet contains one hashtag:\n",
            "\n",
            "* **#EarthDay** \n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Tokenize the hashtags of this tweet:\n",
        "\n",
        "Google Cloud\n",
        "@googlecloud\n",
        "How can data help our changing planet? 🌎\n",
        "\n",
        "In honor of #EarthDay this weekend, we’re proud to share how we’re partnering with\n",
        "@ClimateEngine\n",
        " to harness the power of geospatial data and drive toward a more sustainable future.\n",
        "\n",
        "Check out how → https://goo.gle/3mOUfts\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f-w7mUxJKCT"
      },
      "source": [
        "### Title & heading generation\n",
        "Below, you ask the model to generate five options for possible title/heading combos for a given piece of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWNri4DTJKCU",
        "outputId": "ca8b16e4-dcc5-434f-d85c-189ff76ca086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here are five title options for the text:\n",
            "\n",
            "1. **AI Unlocks Cultural Connections: Google's Latest Experiments in Arts & Culture** (Focuses on the main theme and highlights the experimental nature)\n",
            "2. **From Disease Detection to Decor Inspiration: AI Powers Google's Cultural Lab** (Emphasizes the diverse applications of AI and Google's involvement)\n",
            "3. **Unveiling Hidden Treasures: Google's AI-Powered Tools for Cultural Exploration** (Highlights the discovery aspect and the use of AI)\n",
            "4. **Google's Arts & Culture Lab: Using AI to Connect People with Culture** (Emphasizes the human connection and the lab's mission)\n",
            "5. **Beyond the Archives: AI Helps You Find Art That Matches Your Style** (Focuses on the practical application and the personalization aspect) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Write a title for this text, give me five options:\n",
        "Whether helping physicians identify disease or finding photos of “hugs,” AI is behind a lot of the work we do at Google. And at our Arts & Culture Lab in Paris, we’ve been experimenting with how AI can be used for the benefit of culture.\n",
        "Today, we’re sharing our latest experiments—prototypes that build on seven years of work in partnership the 1,500 cultural institutions around the world.\n",
        "Each of these experimental applications runs AI algorithms in the background to let you unearth cultural connections hidden in archives—and even find artworks that match your home decor.\"\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcpmZnwKJKCU"
      },
      "source": [
        "## Evaluation\n",
        "You can evaluate the outputs from summarization tasks using [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)) as an evalulation framework. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) are measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans.\n",
        "\n",
        "\n",
        "The first step is to install the ROUGE library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJcl38ElJKCU",
        "outputId": "3954b87b-6dd3-408c-dad8-779e159974f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Obtaining dependency information for rouge from https://files.pythonhosted.org/packages/32/7c/650ae86f92460e9e8ef969cc5008b24798dcf56a9a8947d04c78f550b3f5/rouge-1.0.1-py3-none-any.whl.metadata\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: six in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from rouge) (1.16.0)\n",
            "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD9eKq3SJKCU"
      },
      "source": [
        "Create a summary from a language model that you can use to compare against a human-generated summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37m_fb-HJKCU",
        "outputId": "327c02d9-626f-478d-94f3-4aae8ad71225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantum computers use qubits, which are extremely sensitive to errors.  To overcome this, quantum error correction is needed, which encodes information across multiple physical qubits to create a more stable \"logical qubit.\"  This technique is crucial for building large-scale quantum computers capable of performing useful calculations.  By encoding more physical qubits into a single logical qubit, researchers aim to reduce error rates and unlock the potential of quantum algorithms. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "ROUGE = Rouge()\n",
        "\n",
        "prompt = \"\"\"\n",
        "Provide a very short, maximum four sentences, summary for the following article:\n",
        "\n",
        "Our quantum computers work by manipulating qubits in an orchestrated fashion that we call quantum algorithms.\n",
        "The challenge is that qubits are so sensitive that even stray light can cause calculation errors — and the problem worsens as quantum computers grow.\n",
        "This has significant consequences, since the best quantum algorithms that we know for running useful applications require the error rates of our qubits to be far lower than we have today.\n",
        "To bridge this gap, we will need quantum error correction.\n",
        "Quantum error correction protects information by encoding it across multiple physical qubits to form a “logical qubit,” and is believed to be the only way to produce a large-scale quantum computer with error rates low enough for useful calculations.\n",
        "Instead of computing on the individual qubits themselves, we will then compute on logical qubits. By encoding larger numbers of physical qubits on our quantum processor into one logical qubit, we hope to reduce the error rates to enable useful quantum algorithms.\n",
        "\n",
        "Summary:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "candidate = model.generate_content(prompt).text\n",
        "print(candidate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b44f9872e1ba"
      },
      "source": [
        "You will also need a human-generated summary that we will use to compare to the `candidate` generated by the model. We will call this `reference`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0qNdPzOJKCc"
      },
      "outputs": [],
      "source": [
        "reference = \"Quantum computers are sensitive to noise and errors. To bridge this gap, we will need quantum error correction. Quantum error correction protects information by encoding across multiple physical qubits to form a “logical qubit”.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KKaYhzwJKCc"
      },
      "source": [
        "Now you can take the candidate and reference to evaluate the performance. In this case, ROUGE will give you:\n",
        "\n",
        "- `rouge-1`, which measures unigram overlap\n",
        "- `rouge-2`, which measures bigram overlap\n",
        "- `rouge-l`, which measures the longest common subsequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHUH6VuTJKCc",
        "tags": [],
        "outputId": "4cabde27-fa7f-496f-ab10-4988ebcf50ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'rouge-1': {'r': 0.6, 'p': 0.3103448275862069, 'f': 0.40909090459710745},\n",
              "  'rouge-2': {'r': 0.28125,\n",
              "   'p': 0.13043478260869565,\n",
              "   'f': 0.17821781745319093},\n",
              "  'rouge-l': {'r': 0.6, 'p': 0.3103448275862069, 'f': 0.40909090459710745}}]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ROUGE.get_scores(candidate, reference)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "environment": {
      "kernel": "python3",
      "name": "tf2-cpu.2-11.m113",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m113"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}