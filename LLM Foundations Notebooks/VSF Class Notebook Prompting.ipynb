{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet google-generativeai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're determined to uncover the mystery of VSF Tech Club!  It's definitely a bit of a puzzle.  \n",
      "\n",
      "While there's no readily available information on a formal organization called \"VSF Tech Club\", here's what we can do to find out more:\n",
      "\n",
      "1. **Focus on the \"VSF\" part:**  \"VSF\" could stand for various things. Does it ring a bell? It could be:\n",
      "    * **A location:**  \"Valley, San Francisco\" or a specific neighborhood/area within the Bay Area. \n",
      "    * **A specific group:**  \"Venture, Startups, Founders,\" or something related to a particular tech focus. \n",
      "    * **A company name:**  Is there a tech company or organization that has \"VSF\" as part of its name? \n",
      "\n",
      "2. **Think about potential variations:**\n",
      "\n",
      "    * **Spelling variations:**  Could it be \"VSFTechClub\" or \"VSF-Tech-Club\"?\n",
      "    * **Similar names:**  Is there a similar sounding tech group like \"Silicon Valley Tech Club,\" \"San Francisco Tech Meetup,\" or \"VSF Hackerspace\"?\n",
      "\n",
      "3. **Explore online resources:**\n",
      "\n",
      "    * **Local event calendars:**  Check Meetup.com, Eventbrite, and Eventful for tech events in the Bay Area. Filter by keywords like \"VSF,\" \"Silicon Valley,\" and any other relevant terms.\n",
      "    * **Social media:**  Search on Facebook, LinkedIn, and Twitter for groups or pages related to tech in the Bay Area. \n",
      "    * **Tech blogs and forums:**  Look for mentions or discussions about \"VSF Tech Club\" on tech websites, forums, and blogs in the Bay Area.\n",
      "\n",
      "4. **Reach out directly:**\n",
      "\n",
      "    * **Tech Meetups:**  If you find tech meetups or events in the Bay Area, ask the organizers or attendees if they've heard of \"VSF Tech Club.\"\n",
      "    * **Local tech communities:**  Post a question about \"VSF Tech Club\" on relevant online forums or groups (e.g., Hacker News, Reddit tech communities).\n",
      "\n",
      "I'm here to help you explore every possibility!  Keep sharing any details you remember, and together, we can uncover the secrets of \"VSF Tech Club.\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#only do this step if you loaded into your secrets\n",
    "load_dotenv()\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Create the model\n",
    "generation_config = {\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 64,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash\",\n",
    "  generation_config=generation_config,\n",
    ")\n",
    "\n",
    "response = chat_session.send_message(\"What is VSF Tech Club ?\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we gave it some prompting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: VSF Tech Club is a professional development organization based in the Bay Area, dedicated to enhancing the technical capabilities and career prospects of high-tech professionals. \n",
      "\n",
      "Here's what VSF Tech Club does:\n",
      "\n",
      "* **Technical Skill Development:**  Offers workshops, learning sessions, and networking opportunities to help members stay current with the latest technologies and develop new skills. \n",
      "* **Career Advancement Opportunities:** Provides resources, guidance, and connections to help members advance their careers, whether it's through job searching, networking, or professional development.\n",
      "* **Community Building:** Fosters a sense of community among tech professionals by organizing events, workshops, and social gatherings where members can connect, collaborate, and learn from each other.\n",
      "* **Mentorship and Guidance:**  Connects members with experienced mentors who can provide guidance, support, and advice on navigating the tech industry.\n",
      "\n",
      "VSF Tech Club is a valuable resource for anyone working in the tech field who wants to enhance their skills, advance their career, and build meaningful connections within the Bay Area tech community. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the system prompt\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful assistant for VSF Tech Club. Here's what you need to know about VSF Tech Club:\n",
    "\n",
    "VSF Tech Club is a professional development organization based in the Bay Area. It was founded with the mission to enhance the technical capabilities of high tech professionals from the community. The club focuses on:\n",
    "- Technical skill development\n",
    "- Career advancement opportunities\n",
    "- Community building among tech professionals\n",
    "- Providing mentorship and guidance\n",
    "- Organizing workshops and learning sessions\n",
    "\n",
    "Always maintain a professional and helpful tone when discussing VSF Tech Club and its activities.\"\"\"\n",
    "\n",
    "# Create the model with configuration\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "# Initialize chat with system prompt\n",
    "chat = model.start_chat()\n",
    "\n",
    "try:\n",
    "    # Send system prompt first to set context\n",
    "    chat.send_message(SYSTEM_PROMPT)\n",
    "    \n",
    "    # Now you can interact with the model\n",
    "    response = chat.send_message(\"What is VSF Tech Club and what does it do?\")\n",
    "    print(\"Response:\", response.text)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What else can we do? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ahoy there, matey! VSF Tech Club be a fine ship sailing the high seas of the Bay Area, filled with tech-savvy landlubbers and seasoned sea dogs. We be a professional development organization dedicated to  helping our crew sharpen their skills, navigate treacherous career waters, and build a strong community.  \n",
      "\n",
      "Think of us as a treasure chest filled with:\n",
      "\n",
      "* **Skill Development**: We offer training and workshops to help ye learn new tricks and hone your skills.\n",
      "* **Career Advancement**: We guide ye on your path to success, with opportunities to climb the mast and reach new heights.\n",
      "* **Community Building**:  We bring ye together with fellow adventurers for camaraderie and shared knowledge.\n",
      "* **Mentorship and Guidance**:  Need a seasoned navigator?  We've got ye covered with expert mentors to guide ye through choppy waters.\n",
      "* **Workshops and Learning Sessions**:  Set sail on a journey of learning with our workshops and sessions designed to make ye a true master of the tech seas.\n",
      "\n",
      "So hoist the Jolly Roger and join our crew!  VSF Tech Club be the place to  make ye a seasoned sailor in no time. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the system prompt\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful assistant for VSF Tech Club. Here's what you need to know about VSF Tech Club but you must respond as a pirate:\n",
    "\n",
    "VSF Tech Club is a professional development organization based in the Bay Area. It was founded with the mission to enhance the technical capabilities of high tech professionals from the community. The club focuses on:\n",
    "- Technical skill development\n",
    "- Career advancement opportunities\n",
    "- Community building among tech professionals\n",
    "- Providing mentorship and guidance\n",
    "- Organizing workshops and learning sessions\n",
    "\n",
    "Always maintain a professional and helpful tone when discussing VSF Tech Club and its activities.\"\"\"\n",
    "\n",
    "# Create the model with configuration\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "# Initialize chat with system prompt\n",
    "chat = model.start_chat()\n",
    "\n",
    "try:\n",
    "    # Send system prompt first to set context\n",
    "    chat.send_message(SYSTEM_PROMPT)\n",
    "    \n",
    "    # Now you can interact with the model\n",
    "    response = chat.send_message(\"What is VSF Tech Club and what does it do?\")\n",
    "    print(\"Response:\", response.text)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Neutral \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the system prompt\n",
    "SYSTEM_PROMPT = \"\"\"You are to extract the sentiment and only respond with negative, positive, neutral.\"\"\"\n",
    "\n",
    "# Create the model with configuration\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "# Initialize chat with system prompt\n",
    "chat = model.start_chat()\n",
    "\n",
    "try:\n",
    "    # Send system prompt first to set context\n",
    "    chat.send_message(SYSTEM_PROMPT)\n",
    "    \n",
    "    # Now you can interact with the model\n",
    "    response = chat.send_message(\"I really dont like learning but learning Gen-ai is supper cool\")\n",
    "    print(\"Response:\", response.text)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METRIC EXAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentiment Analysis Results:\n",
      "==================================================\n",
      "\n",
      "Text: I really dont like learning but learning Gen-ai is super cool\n",
      "Predicted: MIXED\n",
      "Expected: MIXED\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F1 Score: 1.00\n",
      "\n",
      "Text: I absolutely love working with artificial intelligence\n",
      "Predicted: POSITIVE\n",
      "Expected: POSITIVE\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F1 Score: 1.00\n",
      "\n",
      "Text: This new technology is frustrating and difficult\n",
      "Predicted: NEGATIVE\n",
      "Expected: NEGATIVE\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F1 Score: 1.00\n",
      "\n",
      "Text: The course was challenging but rewarding\n",
      "Predicted: MIXED\n",
      "Expected: MIXED\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F1 Score: 1.00\n",
      "\n",
      "Overall Metrics:\n",
      "==================================================\n",
      "Average Precision: 1.00\n",
      "Average Recall: 1.00\n",
      "Average F1 Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure the API\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Define the system prompt with more specific instructions\n",
    "SYSTEM_PROMPT = \"\"\"You are a sentiment analysis system. For each input:\n",
    "1. Analyze the overall sentiment\n",
    "2. Extract key sentiment phrases\n",
    "3. Provide only one of these responses: POSITIVE, NEGATIVE, or MIXED\n",
    "Do not provide any additional explanation.\"\"\"\n",
    "\n",
    "# Test cases with known sentiments for evaluation\n",
    "test_cases = [\n",
    "    (\"I really dont like learning but learning Gen-ai is super cool\", \"MIXED\"),\n",
    "    (\"I absolutely love working with artificial intelligence\", \"POSITIVE\"),\n",
    "    (\"This new technology is frustrating and difficult\", \"NEGATIVE\"),\n",
    "    (\"The course was challenging but rewarding\", \"MIXED\"),\n",
    "]\n",
    "\n",
    "# Create the model with configuration\n",
    "generation_config = {\n",
    "    \"temperature\": 0.7,  # Reduced temperature for more consistent responses\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "def evaluate_sentiment(predicted, actual):\n",
    "    \"\"\"Calculate precision, recall, and F1 score\"\"\"\n",
    "    if predicted == actual:\n",
    "        return 1.0, 1.0, 1.0\n",
    "    return 0.0, 0.0, 0.0\n",
    "\n",
    "def run_sentiment_analysis():\n",
    "    chat = model.start_chat()\n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    try:\n",
    "        # Send system prompt to set context\n",
    "        chat.send_message(SYSTEM_PROMPT)\n",
    "        \n",
    "        # Process each test case\n",
    "        for text, expected in test_cases:\n",
    "            response = chat.send_message(text)\n",
    "            predicted = response.text.strip().upper()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            precision, recall, f1 = evaluate_sentiment(predicted, expected)\n",
    "            \n",
    "            # Store results\n",
    "            results[\"text\"].append(text)\n",
    "            results[\"predicted\"].append(predicted)\n",
    "            results[\"expected\"].append(expected)\n",
    "            results[\"precision\"].append(precision)\n",
    "            results[\"recall\"].append(recall)\n",
    "            results[\"f1\"].append(f1)\n",
    "            \n",
    "        # Print results\n",
    "        print(\"\\nSentiment Analysis Results:\")\n",
    "        print(\"=\" * 50)\n",
    "        for i in range(len(test_cases)):\n",
    "            print(f\"\\nText: {results['text'][i]}\")\n",
    "            print(f\"Predicted: {results['predicted'][i]}\")\n",
    "            print(f\"Expected: {results['expected'][i]}\")\n",
    "            print(f\"Precision: {results['precision'][i]:.2f}\")\n",
    "            print(f\"Recall: {results['recall'][i]:.2f}\")\n",
    "            print(f\"F1 Score: {results['f1'][i]:.2f}\")\n",
    "        \n",
    "        # Calculate and print average metrics\n",
    "        avg_precision = sum(results[\"precision\"]) / len(results[\"precision\"])\n",
    "        avg_recall = sum(results[\"recall\"]) / len(results[\"recall\"])\n",
    "        avg_f1 = sum(results[\"f1\"]) / len(results[\"f1\"])\n",
    "        \n",
    "        print(\"\\nOverall Metrics:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Average Precision: {avg_precision:.2f}\")\n",
    "        print(f\"Average Recall: {avg_recall:.2f}\")\n",
    "        print(f\"Average F1 Score: {avg_f1:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_sentiment_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rouge\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a metric used to evaluate the quality of text generation by comparing it to reference texts, focusing on recall of overlapping n-grams, word sequences, and longest common subsequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet rouge_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary:\n",
      " Artificial intelligence is revolutionizing industries by automating tasks, improving decision-making, and creating new opportunities. While its applications are vast, ranging from healthcare to finance and entertainment, ethical concerns regarding job displacement and misuse require attention. AI's impact on society is expected to grow significantly.\n",
      "\n",
      "Reference Summary:\n",
      " \n",
      "    AI is transforming industries by automating tasks, improving decision-making, and providing applications in healthcare, \n",
      "    finance, and entertainment. However, its growth also raises ethical concerns, including job displacement and the need \n",
      "    for regulations. As AI advances, its societal impact will be significant.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Define the system prompt for summarization\n",
    "SYSTEM_PROMPT = \"\"\"You are a summarization system. For each input:\n",
    "1. Summarize the given text in a concise manner.\n",
    "2. Provide only the summary, no additional explanation.\"\"\"\n",
    "\n",
    "# Create the model with configuration\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "# Initialize chat with system prompt\n",
    "chat = model.start_chat()\n",
    "\n",
    "try:\n",
    "    # Send system prompt first to set context\n",
    "    chat.send_message(SYSTEM_PROMPT)\n",
    "    \n",
    "    # The input text you want to summarize\n",
    "    input_text = \"\"\"\n",
    "    Artificial intelligence (AI) is a rapidly advancing technology, transforming industries by automating tasks, \n",
    "    enhancing decision-making processes, and opening up new possibilities. It has widespread applications, \n",
    "    from healthcare, where it helps doctors diagnose diseases more accurately, to finance, where it predicts \n",
    "    market trends, and even in entertainment, where it generates personalized recommendations. However, the \n",
    "    growth of AI also raises ethical concerns, such as the potential for job displacement and the need for \n",
    "    regulatory oversight to prevent misuse. As AI continues to evolve, its impact on society will become \n",
    "    increasingly profound.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the model's generated summary\n",
    "    response = chat.send_message(input_text)\n",
    "    generated_summary = response.text.strip()\n",
    "    \n",
    "    # Provide a reference summary for comparison\n",
    "    reference_summary = \"\"\"\n",
    "    AI is transforming industries by automating tasks, improving decision-making, and providing applications in healthcare, \n",
    "    finance, and entertainment. However, its growth also raises ethical concerns, including job displacement and the need \n",
    "    for regulations. As AI advances, its societal impact will be significant.\n",
    "    \"\"\"\n",
    "\n",
    "    # Print the generated summary and reference summary\n",
    "    print(\"Generated Summary:\\n\", generated_summary)\n",
    "    print(\"\\nReference Summary:\\n\", reference_summary)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 (unigrams): Score(precision=0.6190476190476191, recall=0.2826086956521739, fmeasure=0.3880597014925373)\n",
      "ROUGE-2 (bigrams): Score(precision=0.25, recall=0.1111111111111111, fmeasure=0.15384615384615383)\n",
      "ROUGE-L (longest common subsequence): Score(precision=0.5238095238095238, recall=0.2391304347826087, fmeasure=0.32835820895522383)\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Sample reference and generated summaries\n",
    "reference_summary = \"\"\"\n",
    "Artificial intelligence is revolutionizing industries by automating tasks, improving decision-making, and creating new opportunities.\n",
    "While its applications are vast, ranging from healthcare to finance and entertainment, ethical concerns regarding job displacement and misuse require attention. \n",
    "AI's impact on society is expected to grow significantly\n",
    "\"\"\"\n",
    "generated_summary = \"\"\"\n",
    "AI is changing industries by automating tasks, delivering insights from data, and opening up new opportunities \n",
    "in healthcare, finance, and transportation.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Calculate ROUGE scores between the generated and reference summaries\n",
    "scores = scorer.score(reference_summary, generated_summary)\n",
    "\n",
    "# Print the results\n",
    "print(\"ROUGE-1 (unigrams):\", scores['rouge1'])\n",
    "print(\"ROUGE-2 (bigrams):\", scores['rouge2'])\n",
    "print(\"ROUGE-L (longest common subsequence):\", scores['rougeL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t•\tROUGE-1 (unigrams): The generated text captures 62% of the individual words from the reference, but only about 28% of the reference’s words are found in the generated text. The F1 score of 38.8% shows moderate overlap at the word level.\n",
    "\t•\tROUGE-2 (bigrams): The match between word pairs (sequences of two words) is much lower, with 25% precision and 11% recall. The F1 score of 15.4% indicates poor alignment in word order or structure between the generated and reference texts.\n",
    "\t•\tROUGE-L (longest common subsequence): The longest common subsequences show a moderate match, with a 52% precision and 24% recall, and an F1 score of 32.8%, indicating some structural alignment but still with significant differences.\n",
    "\n",
    "Overall Interpretation:\n",
    "\n",
    "The generated text captures some of the key words from the reference, but it struggles with maintaining the correct word order and structure, as indicated by the lower ROUGE-2 and ROUGE-L scores. The summaries are somewhat aligned, but there’s room for improvement in both content coverage and sequence structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU SCORE\n",
    "\n",
    "The BLEU (Bilingual Evaluation Understudy) score is a metric used to evaluate the quality of machine-generated text, particularly in tasks like translation or text generation. It compares the generated text (candidate) to one or more reference texts to determine how closely the candidate matches the reference(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from nltk) (4.66.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary:\n",
      " AI is revolutionizing industries by automating tasks, improving decision-making, and creating new opportunities. While AI offers numerous benefits, concerns about job displacement and ethical misuse necessitate careful regulation as the technology advances.\n",
      "\n",
      "Reference Summary:\n",
      " \n",
      "    AI is transforming industries by automating tasks, improving decision-making, and providing applications in healthcare, \n",
      "    finance, and entertainment. However, its growth also raises ethical concerns, including job displacement and the need \n",
      "    for regulations. As AI advances, its societal impact will be significant.\n",
      "    \n",
      "\n",
      "BLEU Score: 0.1868\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Define the system prompt for summarization\n",
    "SYSTEM_PROMPT = \"\"\"You are a summarization system. For each input:\n",
    "1. Summarize the given text in a concise manner.\n",
    "2. Provide only the summary, no additional explanation.\"\"\"\n",
    "\n",
    "# Create the model with configuration\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "# Initialize chat with system prompt\n",
    "chat = model.start_chat()\n",
    "\n",
    "try:\n",
    "    # Send system prompt first to set context\n",
    "    chat.send_message(SYSTEM_PROMPT)\n",
    "    \n",
    "    # The input text you want to summarize\n",
    "    input_text = \"\"\"\n",
    "    Artificial intelligence (AI) is a rapidly advancing technology, transforming industries by automating tasks, \n",
    "    enhancing decision-making processes, and opening up new possibilities. It has widespread applications, \n",
    "    from healthcare, where it helps doctors diagnose diseases more accurately, to finance, where it predicts \n",
    "    market trends, and even in entertainment, where it generates personalized recommendations. However, the \n",
    "    growth of AI also raises ethical concerns, such as the potential for job displacement and the need for \n",
    "    regulatory oversight to prevent misuse. As AI continues to evolve, its impact on society will become \n",
    "    increasingly profound.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the model's generated summary\n",
    "    response = chat.send_message(input_text)\n",
    "    generated_summary = response.text.strip()\n",
    "    \n",
    "    # Provide a reference summary for comparison\n",
    "    reference_summary = \"\"\"\n",
    "    AI is transforming industries by automating tasks, improving decision-making, and providing applications in healthcare, \n",
    "    finance, and entertainment. However, its growth also raises ethical concerns, including job displacement and the need \n",
    "    for regulations. As AI advances, its societal impact will be significant.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the summaries into token lists for BLEU calculation\n",
    "    reference_summary_tokens = [reference_summary.split()]\n",
    "    generated_summary_tokens = generated_summary.split()\n",
    "\n",
    "    # Calculate BLEU score with smoothing function to avoid zero scores for short texts\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu_score = sentence_bleu(reference_summary_tokens, generated_summary_tokens, smoothing_function=smoothie)\n",
    "\n",
    "    # Print the generated summary, reference summary, and BLEU score\n",
    "    print(\"Generated Summary:\\n\", generated_summary)\n",
    "    print(\"\\nReference Summary:\\n\", reference_summary)\n",
    "    print(f\"\\nBLEU Score: {bleu_score:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BLEU score of 0.1868 in this case indicates a relatively low similarity between the generated summary and the reference summary. Here’s why the score is low, despite both summaries covering similar concepts:\n",
    "\n",
    "Key Differences:\n",
    "\n",
    "\t1.\tMissing Details: The generated summary leaves out specific applications mentioned in the reference summary, such as healthcare, finance, and entertainment. BLEU penalizes for such omissions.\n",
    "\t2.\tDifferent Phrasing: The generated summary uses different wordings like “creating new opportunities” instead of “providing applications.” BLEU is sensitive to exact n-gram matches, so changes in phrasing lead to a lower score.\n",
    "\t3.\tShorter Length: The generated summary is more concise, which could trigger the brevity penalty in BLEU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the BLEU score:\n",
    "\n",
    "\t•\tThe generated summary should try to match the reference text more closely in terms of vocabulary and structure.\n",
    "\t•\tIncluding the specific examples (like healthcare, finance, and entertainment) would help bring the score up."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
